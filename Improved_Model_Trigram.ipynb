{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98e10b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import nltk\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv, re\n",
    "import string\n",
    "import codecs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spacy.lang.en import English\n",
    "from nltk.stem import PorterStemmer\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonlines\n",
    "import spacy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38503080",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/wcpr_mypersonality.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load CSV data\u001b[39;00m\n\u001b[0;32m      3\u001b[0m path_file\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/wcpr_mypersonality.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatin-1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Convert to jsonlines format\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m jsonlines\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmyPersonality.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m writer:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:586\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    571\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    572\u001b[0m     dialect,\n\u001b[0;32m    573\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    583\u001b[0m )\n\u001b[0;32m    584\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:482\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    479\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:811\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 811\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1040\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1037\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown engine: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (valid options are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1038\u001b[0m     )\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m# error: Too many arguments for \"ParserBase\"\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:51\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     48\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# open handles\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Have to pass int, would break tests using TextReader directly otherwise :(\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:222\u001b[0m, in \u001b[0;36mParserBase._open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_handles\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: FilePathOrBuffer, kwds: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m    Let the readers open IOHandles after they are done with their potential raises.\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:702\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    698\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    701\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 702\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    711\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/wcpr_mypersonality.csv'"
     ]
    }
   ],
   "source": [
    "# Load CSV data\n",
    "\n",
    "path_file= r\"data/wcpr_mypersonality.csv\"\n",
    "df = pd.read_csv(path_file, encoding='latin-1')\n",
    "\n",
    "# Convert to jsonlines format\n",
    "with jsonlines.open('myPersonality.jsonl', 'w') as writer:\n",
    "    for _, row in df.iterrows():\n",
    "        writer.write(row.to_dict())\n",
    "        \n",
    "# Load jsonlines data\n",
    "data = []\n",
    "with jsonlines.open('myPersonality.jsonl', 'r') as reader:\n",
    "    for item in reader:\n",
    "        data.append(item)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083b6e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dde9fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "def read_and_clean_lines(df):\n",
    "    print(\"\\nReading and cleaning text from dataframe\")\n",
    "    lines = []\n",
    "    neurotic_flags = []\n",
    "\n",
    "    csv_data = StringIO(df.to_csv(index=False))\n",
    "    reader = csv.reader(csv_data)\n",
    "    next(reader, None)\n",
    "    for row in reader:\n",
    "        lines.append(row[1])\n",
    "        neurotic_flags.append(row[8])\n",
    "\n",
    "    print(\"Read {} status posts.\".format(len(lines)))\n",
    "    print(\"Read {} labels\".format(len(neurotic_flags)))\n",
    "    return lines, neurotic_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e33496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a set of stoplist words from filename, assuming it contains one word per line\n",
    "\n",
    "def load_stopwords(filename):\n",
    "    stopwords = []\n",
    "    with codecs.open(filename, 'r', encoding='ascii', errors='ignore') as fp:\n",
    "        stopwords = fp.read().split('\\n')\n",
    "    return set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de106cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = load_stopwords(path_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bab7967",
   "metadata": {},
   "source": [
    "Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aea0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training items/labels\n",
    "# X_train and y_train  are the training items and labels, respectively\n",
    "# X_test  and y_test   are the test items and labels, respectively\n",
    "def split_training_set(lines, labels, test_size=0.2, random_seed=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(lines, labels, test_size=test_size,stratify=labels,shuffle=True)\n",
    "    print(\"Training set label counts: {}\".format(Counter(y_train)))\n",
    "    print(\"Test set     label counts: {}\".format(Counter(y_test)))\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee83e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset in and split it into training documents/labels (X) and test documents/labels (y)\n",
    "lines, neurotic_flags = read_and_clean_lines(df)\n",
    "X_train, X_test, y_train, y_test = split_training_set(lines, neurotic_flags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0176bc2d",
   "metadata": {},
   "source": [
    "Pass the above stopwords(frequently used words) list as argument to count vectorizer.\n",
    "CountVectorizer tokenizes(tokenization means dividing the sentences in words) the text along with performing very basic preprocessing. It removes the punctuation marks and converts all the words to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c31069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read train(X) data use stopwords, lowercase and ngram_range as arguments\n",
    "def convert_text_into_features(X, stopwords_arg, analyzefn=\"word\", range=(1,2)):\n",
    "    training_vectorizer = CountVectorizer(stop_words=stopwords_arg,\n",
    "                                          analyzer=analyzefn,\n",
    "                                          lowercase=True,\n",
    "                                          ngram_range=range)\n",
    "    X_features = training_vectorizer.fit_transform(X)\n",
    "    return X_features, training_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d981c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to tokenize and normalize\n",
    "def whitespace_tokenizer(line):\n",
    "    return line.split()\n",
    "\n",
    "def normalize_tokens(tokenlist):\n",
    "    normalized_tokens = [token.lower().replace('_','+') for token in tokenlist   # lowercase, _ => +\n",
    "                             if re.search('[^\\s]', token) is not None            # ignore whitespace tokens\n",
    "                             and not token.startswith(\"@\")                       # ignore  handles\n",
    "                        ]\n",
    "    return normalized_tokens  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217d4a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for ngram and filtering stopwords\n",
    "def ngrams(tokens, n):\n",
    "    # Returns all ngrams of size n in sentence, where an ngram is itself a list of tokens\n",
    "    return [tokens[i:i+n] for i in range(len(tokens)-n+1)]\n",
    "\n",
    "def filter_punctuation_bigrams(ngrams):\n",
    "    punct = string.punctuation\n",
    "    return [ngram   for ngram in ngrams   if ngram[0] not in punct and ngram[1] not in punct]\n",
    "\n",
    "# Filters values with punctionation tokens for 3-grams and returns the items that were not removed\n",
    "def filter_punctuation_trigrams(ngrams):\n",
    "    punct = string.punctuation\n",
    "    return [ngram for ngram in ngrams if ngram[0] not in punct and ngram[1] not in punct and ngram[2] not in punct]\n",
    "\n",
    "# Filters values with stop words for 2-grams and returns the items that were not removed\n",
    "def filter_stopword_bigrams(ngrams, stopwords):\n",
    "    result = [ngram   for ngram in ngrams   if ngram[0] not in stopwords and ngram[1] not in stopwords]\n",
    "    return result\n",
    "\n",
    "# Filters values with stop words for 3-grams and returns the items that were not removed\n",
    "def filter_stopword_trigrams(ngrams, stopwords):\n",
    "    result = [ngram for ngram in ngrams if ngram[0] not in stopwords and ngram[1] not in stopwords and ngram[2] not in stopwords]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea01021",
   "metadata": {},
   "source": [
    "Input:\n",
    "lines     - a raw text corpus, where each element in the list is a string\n",
    "stopwords - a set of strings that are stopwords\n",
    "remove_stopword_bigrams = True or False\n",
    "\n",
    "Output:  a corresponding list converting the raw strings to space-separated features\n",
    "\n",
    "The features extracted should include non-stopword, non-punctuation unigrams,\n",
    "plus the bigram features that were counted in collect_bigram_counts from the previous assignment\n",
    "represented as underscore_separated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c73ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert the raw strings to space-separated features \n",
    "\n",
    "def convert_lines_to_feature_strings(lines, stopwords, remove_stopword_ngrams=True, applied_ngrams=[2, 3]):\n",
    "\n",
    "    print(\" Converting from raw text to unigram and bigram features\")\n",
    "    if remove_stopword_ngrams:\n",
    "        print(\" Includes filtering stopword bigrams\")\n",
    "        \n",
    "    print(\" Initializing\")\n",
    "    nlp          = English(parser=False)\n",
    "    all_features = []\n",
    "    print(\" Iterating through documents extracting ngram features\")\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    for line in tqdm(lines):\n",
    "        \n",
    "        # Get spacy tokenization and normalize the tokens\n",
    "        spacy_analysis    = nlp(line)\n",
    "        spacy_tokens      = [token.orth_ for token in spacy_analysis]\n",
    "        normalized_tokens = normalize_tokens(spacy_tokens)\n",
    "        \n",
    "        # Collect unigram tokens as features\n",
    "        # Exclude unigrams that are stopwords or are punctuation strings (e.g. '.' or ',')\n",
    "        unigrams          = [token   for token in normalized_tokens\n",
    "                                 if token not in stopwords and token not in string.punctuation]\n",
    "\n",
    "        # Collect string bigram tokens as features\n",
    "        bigrams = []\n",
    "        bigram_tokens     = [\"_\".join(bigram) for bigram in bigrams]\n",
    "        bigrams           = ngrams(normalized_tokens, 2) \n",
    "        bigrams           = filter_punctuation_bigrams(bigrams)\n",
    "        if remove_stopword_ngrams:\n",
    "            bigrams = filter_stopword_bigrams(bigrams, stopwords)\n",
    "        bigram_tokens = [\"_\".join(bigram) for bigram in bigrams]\n",
    "\n",
    "       # Collect string trigram tokens as features\n",
    "        trigrams = []\n",
    "        trigram_tokens    = [\"_\".join(trigram) for trigram in trigrams]\n",
    "        trigrams          = ngrams(normalized_tokens, 3)\n",
    "        trigrams          = filter_punctuation_trigrams(trigrams)\n",
    "        if remove_stopword_ngrams:\n",
    "            trigrams = filter_stopword_trigrams(trigrams, stopwords)\n",
    "        trigram_tokens = [\"_\".join(trigram) for trigram in trigrams]\n",
    "\n",
    "        feature_string = \" \".join(unigrams) + \" \" + \" \".join(bigram_tokens)\n",
    "        if 3 in applied_ngrams:\n",
    "            feature_string += \" \" + \" \".join(trigram_tokens)\n",
    "        \n",
    "        # Add this feature string to the output\n",
    "        all_features.append(feature_string)\n",
    "        \n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb5b461",
   "metadata": {},
   "source": [
    "Roll your own feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced00d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call convert_lines_to_feature_strings() to get your features\n",
    "# as a whitespace-separated string that will now represent the document.\n",
    "print(\"Creating feature strings for training data\")\n",
    "X_train_feature_strings = convert_lines_to_feature_strings(X_train, stop_words, applied_ngrams=[2, 3])\n",
    "                                                           \n",
    "print(\"Creating feature strings for test data\")\n",
    "X_test_documents = convert_lines_to_feature_strings(X_test,  stop_words, applied_ngrams=[2, 3])\n",
    "    \n",
    "# Call CountVectorizer with whitespace-based tokenization as the analyzer, so that it uses exactly your features,\n",
    "# but without doing any of its own analysis/feature-extraction.\n",
    "X_features_train, training_vectorizer = convert_text_into_features(X_train_feature_strings, stop_words, whitespace_tokenizer)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb4dbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the \"vectorizer\" created using the training data to the test documents, to create testset feature vectors\n",
    "X_test_features =  training_vectorizer.transform(X_test_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535ff7b5",
   "metadata": {},
   "source": [
    "logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f93fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logistic regression classifier trained on the featurized training data\n",
    "lr_classifier = LogisticRegression(solver='liblinear')\n",
    "lr_classifier.fit(X_features_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d225f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the test data and see how well you perform\n",
    "    \n",
    "print(\"Classifying test data...\")\n",
    "predicted_labels = lr_classifier.predict(X_test_features)\n",
    "print('Accuracy  = {}'.format(metrics.accuracy_score(predicted_labels,  y_test)))\n",
    "for label in ['n', 'y']:\n",
    "    print('Precision for label {} = {}'.format(label, metrics.precision_score(predicted_labels, y_test, pos_label=label)))\n",
    "    print('Recall    for label {} = {}'.format(label, metrics.recall_score(predicted_labels, y_test, pos_label=label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659c14b5",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Graph the confusion matrix to show accuracies\n",
    "\n",
    "print(\"Generating plots...\")\n",
    "metrics.plot_confusion_matrix(lr_classifier, X_test_features, y_test, normalize='true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b9b855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
